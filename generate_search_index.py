#!/usr/bin/env python3
"""
ç”Ÿæˆå…¨ç«™æœç´¢ç´¢å¼• JSON
"""

import json
import glob
import re
from bs4 import BeautifulSoup

def extract_text_from_html(html_content):
    """ä»HTMLä¸­æå–çº¯æ–‡æœ¬"""
    soup = BeautifulSoup(html_content, 'html.parser')

    # ç§»é™¤scriptå’Œstyleæ ‡ç­¾
    for script in soup(["script", "style"]):
        script.decompose()

    # è·å–æ–‡æœ¬
    text = soup.get_text()

    # æ¸…ç†æ–‡æœ¬
    lines = (line.strip() for line in text.splitlines())
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
    text = ' '.join(chunk for chunk in chunks if chunk)

    return text

def generate_search_index():
    """ç”Ÿæˆæœç´¢ç´¢å¼•"""
    search_index = []

    # è·å–æ‰€æœ‰HTMLæ–‡ä»¶
    html_files = glob.glob("*.html")

    # æ’é™¤ä¸éœ€è¦çš„æ–‡ä»¶
    excluded_files = {'books-preview.html', 'test.html', 'article1.html'}

    for html_file in html_files:
        if html_file in excluded_files:
            continue

        try:
            with open(html_file, 'r', encoding='utf-8') as f:
                content = f.read()

            soup = BeautifulSoup(content, 'html.parser')

            # æå–æ ‡é¢˜
            title_tag = soup.find('title')
            title = title_tag.text if title_tag else html_file.replace('.html', '')

            # æå–metaæè¿°
            description_tag = soup.find('meta', {'name': 'description'})
            description = description_tag.get('content', '') if description_tag else ''

            # æå–å…³é”®è¯
            keywords_tag = soup.find('meta', {'name': 'keywords'})
            keywords = keywords_tag.get('content', '') if keywords_tag else ''

            # æå–ä¸»è¦å†…å®¹ï¼ˆä¼˜å…ˆä»articleæ ‡ç­¾ï¼‰
            article_content = soup.find('article') or soup.find('main') or soup.find('body')
            if article_content:
                # ç§»é™¤å¯¼èˆªæ å’Œé¡µè„š
                for nav in article_content.find_all(['nav', 'footer']):
                    nav.decompose()

                content_text = extract_text_from_html(str(article_content))
            else:
                content_text = extract_text_from_html(content)

            # é™åˆ¶å†…å®¹é•¿åº¦
            content_preview = content_text[:500] if content_text else description

            # æå–åˆ†ç±»ï¼ˆå¦‚æœæœ‰ï¼‰
            category = ''
            category_tag = soup.find(class_=['blog-tag', 'article-tag'])
            if category_tag:
                category = category_tag.text.strip()

            # æ·»åŠ åˆ°ç´¢å¼•
            search_index.append({
                'url': html_file,
                'title': title.replace(' - è®¡åˆ’æ', '').strip(),
                'description': description or content_preview[:200],
                'category': category,
                'keywords': keywords,
                'content': content_preview
            })

            print(f"âœ… å·²ç´¢å¼•: {html_file} - {title}")

        except Exception as e:
            print(f"âŒ å¤„ç† {html_file} å¤±è´¥: {e}")
            continue

    # ä¿å­˜ç´¢å¼•
    with open('search-index.json', 'w', encoding='utf-8') as f:
        json.dump(search_index, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ‰ æœç´¢ç´¢å¼•å·²ç”Ÿæˆï¼ŒåŒ…å« {len(search_index)} ä¸ªé¡µé¢")

if __name__ == '__main__':
    print("ğŸš€ å¼€å§‹ç”Ÿæˆæœç´¢ç´¢å¼•...")
    generate_search_index()
