#!/usr/bin/env python3
"""
ç”Ÿæˆ sitemap.xml å’Œ robots.txt
"""

import os
from datetime import datetime
import glob

BASE_URL = "https://kev1nl33.github.io/personal-blog"

def generate_sitemap():
    """ç”Ÿæˆ sitemap.xml"""
    # è·å–æ‰€æœ‰ HTML æ–‡ä»¶
    html_files = glob.glob("*.html")

    # æ’é™¤ä¸éœ€è¦çš„æ–‡ä»¶
    excluded_files = {'books-preview.html', 'test.html', 'article1.html'}
    html_files = [f for f in html_files if f not in excluded_files]

    # é¡µé¢ä¼˜å…ˆçº§è®¾ç½®
    priority_map = {
        'index.html': ('1.0', 'daily'),
        'blog.html': ('0.9', 'daily'),
        'about.html': ('0.8', 'monthly'),
        'books.html': ('0.8', 'weekly'),
        'weekly.html': ('0.8', 'weekly'),
    }

    # è·å–æ–‡ä»¶ä¿®æ”¹æ—¶é—´
    def get_lastmod(filepath):
        timestamp = os.path.getmtime(filepath)
        return datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d')

    # ç”Ÿæˆ sitemap XML
    sitemap = ['<?xml version="1.0" encoding="UTF-8"?>']
    sitemap.append('<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">')

    # å…ˆæ·»åŠ ä¸»è¦é¡µé¢
    main_pages = ['index.html', 'blog.html', 'about.html', 'books.html', 'weekly.html']
    for page in main_pages:
        if page in html_files:
            priority, changefreq = priority_map.get(page, ('0.5', 'monthly'))
            lastmod = get_lastmod(page)
            url = f"{BASE_URL}/" if page == 'index.html' else f"{BASE_URL}/{page}"

            sitemap.append('  <url>')
            sitemap.append(f'    <loc>{url}</loc>')
            sitemap.append(f'    <lastmod>{lastmod}</lastmod>')
            sitemap.append(f'    <changefreq>{changefreq}</changefreq>')
            sitemap.append(f'    <priority>{priority}</priority>')
            sitemap.append('  </url>')
            html_files.remove(page)

    # æ·»åŠ å…¶ä»–æ–‡ç« é¡µé¢
    for html_file in sorted(html_files):
        lastmod = get_lastmod(html_file)
        url = f"{BASE_URL}/{html_file}"

        # æ–‡ç« é¡µé¢é»˜è®¤ä¼˜å…ˆçº§
        priority = '0.7'
        changefreq = 'monthly'

        sitemap.append('  <url>')
        sitemap.append(f'    <loc>{url}</loc>')
        sitemap.append(f'    <lastmod>{lastmod}</lastmod>')
        sitemap.append(f'    <changefreq>{changefreq}</changefreq>')
        sitemap.append(f'    <priority>{priority}</priority>')
        sitemap.append('  </url>')

    sitemap.append('</urlset>')

    # å†™å…¥æ–‡ä»¶
    with open('sitemap.xml', 'w', encoding='utf-8') as f:
        f.write('\n'.join(sitemap))

    print(f"âœ… sitemap.xml å·²ç”Ÿæˆï¼ŒåŒ…å« {len(html_files) + len(main_pages)} ä¸ªé¡µé¢")

def generate_robots():
    """ç”Ÿæˆ robots.txt"""
    robots_content = """# robots.txt for è®¡åˆ’æçš„ä¸ªäººåšå®¢

User-agent: *
Allow: /

# Sitemap
Sitemap: https://kev1nl33.github.io/personal-blog/sitemap.xml

# ç¦æ­¢è®¿é—®çš„è·¯å¾„ï¼ˆå¦‚æœæœ‰ï¼‰
Disallow: /scripts/
Disallow: /*.json$
"""

    with open('robots.txt', 'w', encoding='utf-8') as f:
        f.write(robots_content)

    print("âœ… robots.txt å·²ç”Ÿæˆ")

if __name__ == '__main__':
    print("ğŸš€ å¼€å§‹ç”Ÿæˆ SEO æ–‡ä»¶...")
    generate_sitemap()
    generate_robots()
    print("ğŸ‰ å®Œæˆï¼")
